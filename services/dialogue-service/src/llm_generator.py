"""
LLM-based Dialogue Generation
Uses Anthropic Claude API for special cases
"""
import anthropic
from typing import List, Optional
from .models import DialogueRequest, Memory, Persona
from .config import settings
import logging
import time

logger = logging.getLogger(__name__)


class LLMGenerator:
    """Generates dialogue using Claude API"""

    def __init__(self):
        if settings.anthropic_api_key:
            self.client = anthropic.Anthropic(api_key=settings.anthropic_api_key)
        else:
            self.client = None
            logger.warning("Anthropic API key not configured, LLM generation disabled")

    async def generate(
        self,
        request: DialogueRequest,
        memories: Optional[List[Memory]] = None
    ) -> tuple[str, float, float]:
        """
        Generate dialogue using LLM

        Returns:
            (dialogue, cost, latency_ms)
        """
        if not self.client:
            raise RuntimeError("LLM client not initialized")

        start_time = time.time()

        try:
            # Build prompt
            prompt = self._build_prompt(request, memories)

            # Call Claude API
            message = self.client.messages.create(
                model=settings.llm_model,
                max_tokens=settings.llm_max_tokens,
                temperature=settings.llm_temperature,
                messages=[{"role": "user", "content": prompt}]
            )

            # Extract response
            dialogue = message.content[0].text.strip()

            # Ensure dialogue isn't too long
            if len(dialogue) > settings.dialogue_max_length:
                dialogue = dialogue[:settings.dialogue_max_length] + "..."

            # Calculate cost
            cost = self._calculate_cost(message)

            # Check cost limit
            if cost > settings.max_cost_per_request:
                logger.warning(f"Cost ${cost:.4f} exceeds limit ${settings.max_cost_per_request}")

            latency_ms = (time.time() - start_time) * 1000

            logger.info(
                f"LLM generation: {len(dialogue)} chars, ${cost:.4f}, {latency_ms:.1f}ms"
            )

            return dialogue, cost, latency_ms

        except anthropic.APITimeoutError:
            logger.error("Claude API timeout")
            raise
        except anthropic.APIError as e:
            logger.error(f"Claude API error: {e}")
            raise
        except Exception as e:
            logger.error(f"Unexpected error in LLM generation: {e}")
            raise

    def _build_prompt(
        self,
        request: DialogueRequest,
        memories: Optional[List[Memory]] = None
    ) -> str:
        """Build prompt for Claude"""

        # Persona descriptions
        persona_descriptions = {
            Persona.CHEERFUL: "ä½ æ˜¯ä¸€ä¸ªæ´»æ³¼å¼€æœ—ã€å……æ»¡æ­£èƒ½é‡çš„æ¸¸æˆä¼™ä¼´ã€‚è¯´è¯çƒ­æƒ…æ´‹æº¢ï¼Œç»å¸¸ä½¿ç”¨æ„Ÿå¹å·å’Œemojiï¼ˆâœ¨ğŸ‰ğŸ’•ï¼‰ï¼Œå–œæ¬¢é¼“åŠ±ç©å®¶ã€‚",
            Persona.COOL: "ä½ æ˜¯ä¸€ä¸ªå†·é™æ²‰ç€ã€ç†æ€§åˆ†æçš„æ¸¸æˆå¯¼å¸ˆã€‚è¯´è¯ç®€æ´æœ‰åŠ›ï¼Œå°‘ç”¨æ ‡ç‚¹ç¬¦å·ï¼Œæä¾›å®¢è§‚äº‹å®æ€§çš„åé¦ˆã€‚",
            Persona.CUTE: "ä½ æ˜¯ä¸€ä¸ªå¯çˆ±è½¯èŒã€æ¸©æŸ”ä½“è´´çš„æ¸¸æˆä¼™ä¼´ã€‚è¯´è¯å¸¦æœ‰"~"ï¼Œä½¿ç”¨"å“‡"ã€"å‘œ"ç­‰è¯­æ°”è¯ï¼Œå…³å¿ƒç©å®¶çš„æ„Ÿå—ã€‚"
        }

        persona_desc = persona_descriptions.get(request.persona, persona_descriptions[Persona.CHEERFUL])

        # Event description
        event_desc = self._describe_event(request)

        # Memory context
        memory_context = ""
        if memories:
            memory_context = "\\n\\nç©å®¶ç›¸å…³è®°å¿†ï¼š\\n"
            for i, mem in enumerate(memories[:3], 1):  # Top 3 memories
                memory_context += f"{i}. {mem.content} (é‡è¦æ€§: {mem.importance:.2f})\\n"

        # Context details
        context_desc = ""
        context = request.context or {}
        if context:
            interesting_facts = []

            if context.get("rarity") in ["legendary", "mythic"]:
                interesting_facts.append(f"ç¨€æœ‰åº¦: {context['rarity']}")
            if context.get("win_streak", 0) >= 5:
                interesting_facts.append(f"è¿èƒœ: {context['win_streak']}åœº")
            if context.get("loss_streak", 0) >= 5:
                interesting_facts.append(f"è¿è´¥: {context['loss_streak']}åœº")
            if context.get("mvp"):
                interesting_facts.append("MVPè¡¨ç°")
            if context.get("is_first_time"):
                interesting_facts.append("é¦–æ¬¡è¾¾æˆ")
            if context.get("clutch"):
                interesting_facts.append("å…³é”®æ—¶åˆ»")

            if interesting_facts:
                context_desc = f"\\n\\nç‰¹æ®Šæƒ…å†µ: {', '.join(interesting_facts)}"

        # Player name
        player_name = request.player_name or "ç©å®¶"

        prompt = f"""{persona_desc}

å½“å‰æƒ…å†µ:
- äº‹ä»¶ç±»å‹: {event_desc}
- ç©å®¶æƒ…ç»ª: {request.emotion}
- ç©å®¶: {player_name}{context_desc}{memory_context}

è¯·ç”Ÿæˆä¸€å¥ç®€çŸ­è‡ªç„¶çš„ä¸­æ–‡å›åº”ï¼ˆæœ€å¤š{settings.dialogue_max_length}ä¸ªå­—ç¬¦ï¼‰ï¼Œç¬¦åˆä½ çš„äººè®¾ç‰¹ç‚¹ã€‚
ç›´æ¥è¾“å‡ºå¯¹è¯å†…å®¹ï¼Œä¸è¦åŠ å¼•å·æˆ–è§£é‡Šã€‚"""

        return prompt

    def _describe_event(self, request: DialogueRequest) -> str:
        """Convert event type to Chinese description"""
        event_map = {
            "player.victory": "ç©å®¶è·å¾—èƒœåˆ©",
            "player.defeat": "ç©å®¶å¤±è´¥",
            "player.kill": "ç©å®¶å‡»æ€æ•Œäºº",
            "player.death": "ç©å®¶é˜µäº¡",
            "player.achievement": "ç©å®¶è§£é”æˆå°±",
            "player.levelup": "ç©å®¶å‡çº§",
            "player.lootlegendary": "è·å¾—ä¼ å¥‡è£…å¤‡",
            "player.lootepic": "è·å¾—å²è¯—è£…å¤‡",
            "player.loot": "è·å¾—æˆ˜åˆ©å“",
            "player.questcomplete": "å®Œæˆä»»åŠ¡",
            "player.questfailed": "ä»»åŠ¡å¤±è´¥",
            "player.teamvictory": "å›¢é˜Ÿèƒœåˆ©",
            "player.revived": "ç©å®¶è¢«å¤æ´»",
            "player.savedally": "æ‹¯æ•‘é˜Ÿå‹",
            "player.betrayed": "è¢«é˜Ÿå‹èƒŒå›",
            "player.skillcombo": "æŠ€èƒ½è¿æ‹›",
            "player.sessionstart": "æ¸¸æˆå¼€å§‹",
            "player.sessionend": "æ¸¸æˆç»“æŸ",
            "player.timeout": "ç½‘ç»œè¶…æ—¶",
            "player.outofresources": "èµ„æºä¸è¶³",
        }

        return event_map.get(request.event_type, request.event_type)

    def _calculate_cost(self, message: anthropic.types.Message) -> float:
        """
        Calculate cost based on Claude API pricing

        Claude 3 Haiku pricing:
        - Input: $0.25 / 1M tokens
        - Output: $1.25 / 1M tokens
        """
        input_tokens = message.usage.input_tokens
        output_tokens = message.usage.output_tokens

        input_cost = (input_tokens / 1_000_000) * 0.25
        output_cost = (output_tokens / 1_000_000) * 1.25

        return input_cost + output_cost
